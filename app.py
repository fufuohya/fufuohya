# -*- coding: utf-8 -*-
import io
import re
import time
import csv
import pandas as pd
import streamlit as st

from shutil import which
from typing import List, Optional

# Selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options

# Optional fallback if apt-installed chromedriver is missing
try:
    from webdriver_manager.chrome import ChromeDriverManager
    from webdriver_manager.core.os_manager import ChromeType
    HAS_WDM = True
except Exception:
    HAS_WDM = False

APP_TITLE = "ğŸ” COSING æˆåˆ†æœå°‹å·¥å…·ï¼ˆStreamlit é›²ç«¯ç‰ˆï¼‰"

st.set_page_config(page_title="COSING Helper", layout="wide")
st.title(APP_TITLE)
st.caption("æŠŠåŸæœ¬çš„ Selenium è…³æœ¬å°è£åˆ° Streamlitï¼Œæ”¯æ´ä¸Šå‚³ï¼è²¼ä¸Šï¼ç¶²å€ï¼Google Sheet ç­‰å¤šç¨®è¼¸å…¥ï¼Œä¸¦å¯ä¸‹è¼‰ CSV çµæœã€‚")

with st.expander("âš ï¸ ä½¿ç”¨å‰æ³¨æ„äº‹é …", expanded=False):
    st.markdown(
        """
- è«‹åˆç†è¨­å®š**æ¯ç­†æŸ¥è©¢å»¶é²**ï¼Œé¿å…éåº¦é »ç¹è«‹æ±‚ç›®æ¨™ç¶²ç«™ã€‚
- è‹¥å…¬å¸ç¶²è·¯æœ‰ä»£ç†ï¼ˆproxyï¼‰æˆ–é˜²ç«ç‰†ï¼Œè«‹åœ¨å·¦å´**Proxy**æ¬„ä½å¡«å…¥ï¼ˆä¾‹å¦‚ `http://user:pass@host:port`ï¼‰ã€‚
- è‹¥é‡åˆ°å½ˆå‡ºè¦–çª—ï¼ˆcookies/åŒæ„ï¼‰ï¼Œç¨‹å¼æœƒå˜—è©¦è‡ªå‹•é»æ“Šï¼›è‹¥ç¶²ç«™æ¨£å¼æ”¹å‹•ï¼Œè«‹å›å ±ä»¥ä¾¿æ›´æ–°é¸æ“‡å™¨ã€‚
        """
    )

# ---------------------------------
# Sidebar - è¨­å®š
# ---------------------------------
st.sidebar.header("è¨­å®š")
headless = st.sidebar.checkbox("Headlessï¼ˆèƒŒæ™¯åŸ·è¡Œç€è¦½å™¨ï¼‰", value=True)
delay = st.sidebar.slider("æ¯ç­†æŸ¥è©¢å»¶é²ï¼ˆç§’ï¼‰", 0.5, 5.0, 1.0, 0.5)
proxy = st.sidebar.text_input("HTTP(S) Proxyï¼ˆé¸å¡«ï¼‰", value="")
st.sidebar.markdown("---")
st.sidebar.caption("è‹¥é‡å…ƒç´ æŠ“ä¸åˆ° â†’ æé«˜å»¶é² / æ”¾æ…¢æ“ä½œã€‚")

# ---------------------------------
# è¼¸å…¥ä¾†æº
# ---------------------------------
st.subheader("è¼¸å…¥æˆåˆ†æ¸…å–®")
uploaded = st.file_uploader("ä¸Šå‚³æª”æ¡ˆï¼ˆæ”¯æ´ .txt / .csv / .xlsxï¼‰", type=["txt", "csv", "xlsx"])
text_input = st.text_area("æˆ–ç›´æ¥è²¼ä¸Šï¼ˆæ¯è¡Œä¸€å€‹æˆåˆ†ï¼‰", height=180, placeholder="ä¾‹å¦‚ï¼š\nWater\nGlycerin\nNiacinamide")

col1, col2 = st.columns(2)
with col1:
    sheet_url = st.text_input("Google Sheet é€£çµï¼ˆå…¬é–‹å¯è®€ï¼‰", placeholder="https://docs.google.com/spreadsheets/d/...")
with col2:
    data_url = st.text_input("é ç«¯è³‡æ–™æª”ç¶²å€ï¼ˆ.txt æˆ– .csvï¼‰", placeholder="https://.../ingredients.txt æˆ– ingredients.csv")

def parse_ingredients_from_upload(file) -> List[str]:
    items: List[str] = []
    if file is None:
        return items
    name = file.name.lower()
    try:
        if name.endswith(".txt"):
            content = file.read().decode("utf-8", errors="ignore")
            items = [ln.strip() for ln in content.splitlines() if ln.strip()]
        elif name.endswith(".csv"):
            df = pd.read_csv(file)
            col = "Ingredient" if "Ingredient" in df.columns else df.columns[0]
            items = [str(v).strip() for v in df[col].dropna().tolist()]
        elif name.endswith(".xlsx"):
            df = pd.read_excel(file)  # éœ€è¦ openpyxl
            col = "Ingredient" if "Ingredient" in df.columns else df.columns[0]
            items = [str(v).strip() for v in df[col].dropna().tolist()]
    except Exception as e:
        st.error(f"è®€å–ä¸Šå‚³æª”æ¡ˆå¤±æ•—ï¼š{e}")
    return items

def gsheet_to_csv_url(url: str) -> str:
    # è½‰æ› Google Sheet æª¢è¦–é€£çµç‚º CSV åŒ¯å‡ºé€£çµ
    m = re.match(r"https://docs\.google\.com/spreadsheets/d/([^/]+)/(?:edit|view).*?[#&]gid=(\d+)", url)
    if m:
        file_id, gid = m.group(1), m.group(2)
        return f"https://docs.google.com/spreadsheets/d/{file_id}/export?format=csv&gid={gid}"
    m2 = re.match(r"https://docs\.google\.com/spreadsheets/d/([^/]+)", url)
    if m2:
        file_id = m2.group(1)
        return f"https://docs.google.com/spreadsheets/d/{file_id}/export?format=csv"
    raise ValueError("ç„¡æ³•è§£ææ­¤ Google Sheet é€£çµ")

def parse_ingredients_from_gsheet(url: str) -> List[str]:
    if not url.strip():
        return []
    try:
        csv_url = gsheet_to_csv_url(url.strip())
        df = pd.read_csv(csv_url)
        col = "Ingredient" if "Ingredient" in df.columns else df.columns[0]
        return [str(v).strip() for v in df[col].dropna().tolist()]
    except Exception as e:
        st.error(f"è®€å– Google Sheet å¤±æ•—ï¼š{e}")
        return []

def parse_ingredients_from_url(url: str) -> List[str]:
    if not url.strip():
        return []
    try:
        if url.lower().endswith(".txt"):
            import urllib.request
            with urllib.request.urlopen(url) as resp:
                content = resp.read().decode("utf-8", errors="ignore")
            return [ln.strip() for ln in content.splitlines() if ln.strip()]
        else:  # å‡è¨­ csv
            df = pd.read_csv(url)
            col = "Ingredient" if "Ingredient" in df.columns else df.columns[0]
            return [str(v).strip() for v in df[col].dropna().tolist()]
    except Exception as e:
        st.error(f"è®€å–ç¶²å€å¤±æ•—ï¼š{e}")
        return []

def merge_dedup(*lists: List[str]) -> List[str]:
    seen = set()
    out = []
    for lst in lists:
        for it in lst:
            if it and it not in seen:
                seen.add(it)
                out.append(it)
    return out

ingredients = merge_dedup(
    parse_ingredients_from_upload(uploaded),
    [ln.strip() for ln in text_input.splitlines() if ln.strip()] if text_input else [],
    parse_ingredients_from_gsheet(sheet_url),
    parse_ingredients_from_url(data_url),
)

st.write(f"å·²è¼‰å…¥ **{len(ingredients)}** å€‹æˆåˆ†ã€‚")

# ---------------------------------
# Driver å»ºç½®ï¼ˆé›²ç«¯ç›¸å®¹ï¼‰
# ---------------------------------
def build_driver(headless: bool = True, proxy_url: str = "", custom_path: Optional[str] = None):
    options = Options()
    if headless:
        options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-gpu")
    options.add_experimental_option("excludeSwitches", ["enable-logging"])

    if proxy_url.strip():
        options.add_argument(f"--proxy-server={proxy_url.strip()}")

    # å„ªå…ˆï¼šapt å®‰è£çš„ chromium/chromedriverï¼ˆStreamlit Cloudï¼‰
    chromium_bin = which("chromium") or which("chromium-browser") or which("google-chrome")
    if chromium_bin:
        options.binary_location = chromium_bin
    chromedriver_bin = which("chromedriver")

    try:
        if custom_path:  # ä½¿ç”¨è€…æŒ‡å®š
            service = Service(executable_path=custom_path)
            return webdriver.Chrome(service=service, options=options)
        if chromedriver_bin:  # ç³»çµ±å·²æœ‰ driver
            service = Service(executable_path=chromedriver_bin)
            return webdriver.Chrome(service=service, options=options)
        # å¾Œå‚™ï¼šwebdriver-manager ä¸‹è¼‰å°æ‡‰é©…å‹•ï¼ˆä»¥ CHROMIUM é¡å‹å„ªå…ˆï¼‰
        if HAS_WDM:
            service = Service(ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install())
            return webdriver.Chrome(service=service, options=options)
        # æœ€å¾Œï¼šSelenium Managerï¼ˆSelenium 4.6+ï¼‰
        return webdriver.Chrome(options=options)
    except Exception as e:
        raise RuntimeError(f"å•Ÿå‹• Chrome å¤±æ•—ï¼š{e}")

# ---------------------------------
# Scraper
# ---------------------------------
def scrape_one(driver, ingredient: str, wait_sec: int = 25):
    wait = WebDriverWait(driver, wait_sec)

    # ç­‰æœå°‹æ¡†å‡ºç¾ä¸¦è¼¸å…¥
    search_box = wait.until(EC.presence_of_element_located((By.ID, "keyword")))
    search_box.clear()
    search_box.send_keys(ingredient)

    # é»æ“Šæœå°‹
    search_button = wait.until(
        EC.element_to_be_clickable((By.XPATH, "//button[@type='submit' and contains(@class, 'ecl-button--primary')]"))
    )
    driver.execute_script("arguments[0].click();", search_button)

    # ç­‰è¡¨æ ¼æ¸²æŸ“
    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table")))

    # å–ç¬¬ä¸€åˆ—
    rows = driver.find_elements(By.CSS_SELECTOR, "table tr")
    if len(rows) > 1:
        cells = rows[1].find_elements(By.TAG_NAME, "td")
        if len(cells) >= 5:
            inci_name = cells[1].text.strip()
            cas_number = cells[2].text.strip()
            annex_ref = cells[4].text.strip()
            return {"Ingredient": ingredient, "INCI Name": inci_name, "CAS Number": cas_number, "Annex / Ref": annex_ref}

    # ç„¡çµæœ
    return {"Ingredient": ingredient, "INCI Name": "No Results", "CAS Number": "No Results", "Annex / Ref": "No Results"}

def try_close_cookie_banner(driver):
    # å˜—è©¦é—œé–‰å¸¸è¦‹çš„åŒæ„å½ˆçª—
    try:
        btn = WebDriverWait(driver, 5).until(
            EC.element_to_be_clickable((By.XPATH, "//button[contains(., 'Accept') or contains(., 'åŒæ„') or contains(., 'æ¥å—')]"))
        )
        btn.click()
    except Exception:
        pass

# ---------------------------------
# ä¸»æµç¨‹
# ---------------------------------
start = st.button("ğŸš€ é–‹å§‹æœå°‹")
results_df = None

if start:
    if not ingredients:
        st.warning("è«‹å…ˆæä¾›æˆåˆ†æ¸…å–®ï¼ˆä¸Šå‚³/è²¼ä¸Š/Google Sheet/ç¶²å€ï¼‰ã€‚")
        st.stop()

    status = st.empty()
    progress = st.progress(0)
    table_ph = st.empty()

    try:
        driver = build_driver(headless=headless, proxy_url=proxy)
        driver.set_page_load_timeout(60)

        url = "https://ec.europa.eu/growth/tools-databases/cosing/"
        driver.get(url)

        try_close_cookie_banner(driver)

        collected = []
        total = len(ingredients)

        for idx, ing in enumerate(ingredients, start=1):
            status.info(f"æœå°‹ç¬¬ {idx}/{total} å€‹ï¼š**{ing}**")
            try:
                data = scrape_one(driver, ing)
            except Exception as e:
                data = {"Ingredient": ing, "INCI Name": "Error", "CAS Number": "Error", "Annex / Ref": f"Error: {e}"}
            collected.append(data)

            progress.progress(int(idx * 100 / total))
            table_ph.dataframe(pd.DataFrame(collected), use_container_width=True)
            time.sleep(delay)

        results_df = pd.DataFrame(collected)
        st.success("å®Œæˆï¼")
        st.dataframe(results_df, use_container_width=True)

        # ä¸‹è¼‰ CSV
        csv_buf = io.StringIO()
        results_df.to_csv(csv_buf, index=False, encoding="utf-8-sig")
        st.download_button("â¬‡ï¸ ä¸‹è¼‰ CSV", csv_buf.getvalue(), file_name="cosing_results.csv", mime="text/csv")

    except Exception as e:
        st.error(f"å•Ÿå‹•ç€è¦½å™¨æˆ–æŠ“å–æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

    finally:
        try:
            driver.quit()
        except Exception:
            pass

st.markdown("---")
st.markdown("Â© 2025 COSING Helper â€” Selenium + Streamlitï¼ˆCommunity Cloud ç›¸å®¹ç‰ˆï¼‰")
